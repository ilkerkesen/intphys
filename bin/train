#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import os.path as osp
from copy import deepcopy

import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.utils.data import DataLoader
import pytorch_lightning as pl
import hydra
from omegaconf import OmegaConf

from intphys import *
from intphys.data import train_collate_fn as collate_fn
from intphys.data import *
from intphys.experiment import Experiment, Overfit


CONFIG_DIR = osp.abspath(osp.join(__file__, "../..", "config"))


def adapt2data(config, dataset):
    if config["model"].get("input_size") is None:
        config["model"]["input_size"] = len(dataset.question_vocab)
    if config["model"].get("output_size") is None:
        config["model"]["output_size"] = len(dataset.answer_vocab)
    if config["model"].get("depth_size") is None:
        config["model"]["depth_size"] = dataset.fps * dataset.NUM_SECONDS
    return config


@hydra.main(config_path=CONFIG_DIR, config_name="train")
def train(config):
    config = OmegaConf.to_container(config)
    config = pl.utilities.parsing.AttributeDict(config)
    pl.seed_everything(config["seed"])

    # load data
    dataset_class = eval(config["dataset"]["name"])
    train_data = dataset_class(split="train", **config["dataset"]["params"])
    val_data = dataset_class(split="validation", **config["dataset"]["params"])

    # logger
    config["logger"]["save_dir"] = osp.abspath(
        osp.expanduser(config["logger"]["save_dir"]))
    if config["logger"]["name"] is None:
        config["logger"]["name"] = config["model"]["architecture"]
    logger = pl.loggers.TensorBoardLogger(**config["logger"])

    # checkpoint
    checkpoints_path = osp.join(logger.log_dir, "checkpoints")
    config["checkpoint"]["filepath"] = osp.join(checkpoints_path, "{epoch:03d}")
    checkpoint_callback = pl.callbacks.ModelCheckpoint(**config["checkpoint"])
    last_ckpt = osp.join(checkpoints_path, "last.ckpt")
    last_ckpt = last_ckpt if osp.isfile(last_ckpt) else None
    ckpt_path = config["trainer"]["resume_from_checkpoint"]

    if last_ckpt is not None and ckpt_path is not None:
        raise Exception("resume checkpoint passed (last.ckpt exists already)")
    
    if config["trainer"]["resume_from_checkpoint"] is None:
        config["trainer"]["resume_from_checkpoint"] = ckpt_path = last_ckpt

    if ckpt_path is not None and not osp.isfile(ckpt_path):
        raise Exception("ckpt does not exist at {}".format(
            config["trainer"]["resume_from_checkpoint"]))

    # adapt model to dataset
    config = adapt2data(deepcopy(config), train_data)

    # initialize experiment
    ThisExperiment = Experiment if not config["overfit"] else Overfit
    experiment = ThisExperiment(config)

    # create data loaders
    train_data.adapt2model(experiment.model)
    train_dataloader = DataLoader(
        train_data,
        shuffle=True,
        collate_fn=collate_fn,
        **config["loader"])

    val_dataloader = None
    if not config["overfit"]:
        val_data.adapt2model(experiment.model)
        val_dataloader = DataLoader(
            val_data,
            shuffle=False,
            collate_fn=collate_fn,
            **config["loader"])

    # create trainer object
    trainer = pl.Trainer(
        logger=logger,
        callbacks=[checkpoint_callback],
        **config["trainer"])

    # overfit
    if config["overfit"]:
        trainer.fit(experiment, train_dataloader=train_dataloader)

    # training
    if config["train"] and not config["overfit"]:
        trainer.fit(
            experiment,
            train_dataloader=train_dataloader,
            val_dataloaders=val_dataloader)

    # test
    if config["test"] and not config["overfit"]:
        test_data = IntuitivePhysicsDataset(split="test", **config["dataset"])
        test_data.adapt2model(experiment.model)
        test_dataloader = DataLoader(
            test_data,
            shuffle=False,
            collate_fn=collate_fn,
            **config["loader"])
        trainer.test(test_dataloaders=test_dataloader)


if __name__ == "__main__":
    train()
