#!/usr/bin/env python

import os
import os.path as osp
import click
from copy import deepcopy
from itertools import product

QUESTION_CATEGORIES = {
    # 'C/A': (0, 1),  # cause, action
    # 'C/N': (3, 4),  # cause, number

    # 'E/A': (0, 1),  # enable, action
    # 'E/N': (3, 4),  # enable, action

    # 'P/A': (0, 1),  # prevent, action
    # 'P/N': (3, 4),  # prevent, number
    
    'C/A': (0, 1),
    'C/N': (3, 4),

    'CF/N': (3, 4),  # counterfactual, number
    'CF/O': (0, 1, 6, 7),  # counterfactual, objects

    'D/C': (22, 23),  # description, color
    'D/S': (24, 25),  # description, shape
    'D/2Qs': (0, 1, 2, 3),  # description, two questions
    'D/N-V': (4, 5, 6, 7, 8, 9, 10, 11),  # description, number-verb
    'D/N-T': (16, 17, 18, 19),  # description, temporal sequence
    'D/TO': (38, 39),  # description, temporal order
    'D/C-T': (26, 27, 28, 29, 32, 33, 34, 35),  # cause - temporal seq.
}


def parse_line(l):
    return l.strip().split('\t')


def read_headers(f):
    return parse_line(f.readline().replace("question_type", "template"))


def parse_int(k, v):
    return int(v) if k.endswith("_index") else v


def lines2records(headers, lines):
    records = {}
    for entry in lines:
        try:
            vid, qid = map(int, entry[:2])
        except ValueError:
            1 == 1
        record = {k:parse_int(k,v) for (k,v) in zip(headers[2:], entry[2:])}
        records[vid,qid] = record
    return records


def read_file(f):
    headers = read_headers(f)
    lines = [parse_line(l) for l in f.readlines()]
    f.close()
    return lines2records(headers, lines)


def eval_attribute_scores(records, attribute, header_type=str,
                          merge_categories=True):
    num_correct, num_examples = dict(), dict()
    for (k, v) in records.items():
        if attribute in v.keys():
            attr_val = header_type(v[attribute])
        elif attribute == "question_category":
            attr_val = header_type(find_question_cat(v["template_id"]))
            
        if attr_val in ("enable", "prevent") and merge_categories:
            attr_val = "cause"
        num_examples[attr_val] = 1 + num_examples.get(attr_val, 0)
        if v["answer"] == v["prediction"]:
            num_correct[attr_val] = 1 + num_correct.get(attr_val, 0)
    headers = sorted(list(num_examples.keys()))
    scores = [num_correct.get(k, 0) / num_examples[k] for k in headers]
    scores = [round(100 * score, 2) for score in scores]
    overall_acc = sum(num_correct.values()) / sum(num_examples.values())
    overall_acc = round(100 * overall_acc, 2)
    headers.append("overall")
    scores.append(overall_acc)
    print(" & ".join(map(str, headers)))
    print(" & ".join(map(str, scores)))


def find_question_cat(template_id):
    task, tid = template_id.split("_")
    task, tid = task.lower(), int(tid)
    if task in ("enable", "prevent"):
        task = "cause"
    task = task[0].upper() if task != "counterfactual" else "CF"
    keys = [k for k in QUESTION_CATEGORIES.keys() if k.startswith(task)]
    for k in keys:
        if tid in QUESTION_CATEGORIES[k]:
            return k
    return -1


@click.command()
@click.argument("answers-file", type=click.File("r"))
@click.argument("predictions-file", type=click.File("r"))
@click.option("-d", "--delimiter", default="tab")
def main(answers_file, predictions_file, delimiter):
    delimiter = "\t" if delimiter == "tab" else delimiter
    delimiter = "," if delimiter == "comma" else delimiter
    answers = read_file(answers_file)
    predictions = read_file(predictions_file)
    assert sorted(answers.keys()) == sorted(predictions.keys())
    merged = deepcopy(answers)
    for (vid, qid) in merged.keys():
        merged[vid,qid].update(predictions.get((vid,qid)))

    num_correct = 0
    for (k, v) in merged.items():
        if v["prediction"] == v["answer"]:
            num_correct += 1
    overall_acc = round(100 * num_correct / len(merged), 2)
    print("accuracy={}%\n".format(overall_acc))

    eval_attribute_scores(merged, "template"); print()
    eval_attribute_scores(merged, "simulation_id", int)
    eval_attribute_scores(merged, "question_category")


if __name__ == "__main__":
    main()
